{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.5.1)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn imbalanced-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values and encoding categorical variables.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop('target', axis=1)  # Replace 'target' with your actual target column name\n",
    "    y = data['target']\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Define transformers for numerical and categorical data\n",
    "    numerical_transformer = SimpleImputer(strategy='mean')  # Impute missing values with mean\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing with mode\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))     # Encode categorical variables\n",
    "    ])\n",
    "\n",
    "    # Create a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply transformations\n",
    "    X_clean = preprocessor.fit_transform(X)\n",
    "\n",
    "    return X_clean, y, preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression\n",
    "\n",
    "def engineer_features(X, y, task_type):\n",
    "    \"\"\"\n",
    "    Scale features and perform feature selection based on the task type.\n",
    "    \"\"\"\n",
    "    # Scaling numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Feature Selection\n",
    "    if task_type == 'classification':\n",
    "        selector = SelectKBest(score_func=f_classif, k='all')  # Adjust k as needed\n",
    "    elif task_type == 'regression':\n",
    "        selector = SelectKBest(score_func=f_regression, k='all')  # Adjust k as needed\n",
    "    else:\n",
    "        selector = None  # For clustering, feature selection might be different\n",
    "\n",
    "    if selector:\n",
    "        X_selected = selector.fit_transform(X_scaled, y)\n",
    "    else:\n",
    "        X_selected = X_scaled\n",
    "\n",
    "    return X_selected, scaler, selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_and_train_model(X, y, task_type):\n",
    "    \"\"\"\n",
    "    Select and train a model based on the task type.\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Select model\n",
    "    if task_type == 'classification':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    elif task_type == 'regression':\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "    elif task_type == 'clustering':\n",
    "        model = KMeans(n_clusters=3, random_state=42)  # Number of clusters can be parameterized\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported task type\")\n",
    "\n",
    "    # Train the model\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        model.fit(X_train, y_train)\n",
    "    elif task_type == 'clustering':\n",
    "        model.fit(X)\n",
    "\n",
    "    # Return relevant data\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        return model, X_test, y_test\n",
    "    else:\n",
    "        return model, X, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score, silhouette_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, task_type):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model and return relevant metrics.\n",
    "    \"\"\"\n",
    "    if task_type == 'classification':\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "            'F1 Score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        }\n",
    "    elif task_type == 'regression':\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = {\n",
    "            'Mean Squared Error': mean_squared_error(y_test, y_pred),\n",
    "            'Mean Absolute Error': mean_absolute_error(y_test, y_pred),\n",
    "            'R2 Score': r2_score(y_test, y_pred)\n",
    "        }\n",
    "    elif task_type == 'clustering':\n",
    "        # For clustering, since there's no ground truth, use silhouette score if possible\n",
    "        y_pred = model.labels_\n",
    "        metrics = {\n",
    "            'Silhouette Score': silhouette_score(X_test, y_pred)\n",
    "        }\n",
    "    else:\n",
    "        metrics = {}\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_ml_pipeline(file_path, task_type):\n",
    "    \"\"\"\n",
    "    Automated ML pipeline that takes a data file and task type, and returns evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Load Data\n",
    "    data = load_data(file_path)\n",
    "    \n",
    "    # Step 2: Clean Data\n",
    "    X_clean, y, preprocessor = clean_data(data)\n",
    "    \n",
    "    # Step 3: Feature Engineering\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        X_engineered, scaler, selector = engineer_features(X_clean, y, task_type)\n",
    "    else:\n",
    "        X_engineered = X_clean  # For clustering, feature selection might differ\n",
    "    \n",
    "    # Step 4: Model Selection and Training\n",
    "    model, X_test, y_test = select_and_train_model(X_engineered, y, task_type)\n",
    "    \n",
    "    # Step 5: Evaluation\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        metrics = evaluate_model(model, X_test, y_test, task_type)\n",
    "    elif task_type == 'clustering':\n",
    "        metrics = evaluate_model(model, X_engineered, None, task_type)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def create_pipeline(task_type):\n",
    "    \"\"\"\n",
    "    Create a machine learning pipeline based on the task type.\n",
    "    \"\"\"\n",
    "    if task_type == 'classification':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        scoring = 'accuracy'\n",
    "    elif task_type == 'regression':\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        scoring = 'r2'\n",
    "    elif task_type == 'clustering':\n",
    "        model = KMeans(n_clusters=3, random_state=42)\n",
    "        scoring = 'silhouette'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported task type\")\n",
    "    \n",
    "    # Define the pipeline steps\n",
    "    steps = [\n",
    "        ('preprocessor', preprocessor),  # From the clean_data function\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ]\n",
    "    \n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    return pipeline, scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X, y, task_type, scoring):\n",
    "    \"\"\"\n",
    "    Evaluate the pipeline using cross-validation.\n",
    "    \"\"\"\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        scores = cross_val_score(pipeline, X, y, cv=5, scoring=scoring)\n",
    "        metrics = {\n",
    "            f'Cross-Validation {scoring.capitalize()} Scores': scores,\n",
    "            f'Mean {scoring.capitalize()}': scores.mean(),\n",
    "            f'Standard Deviation': scores.std()\n",
    "        }\n",
    "    elif task_type == 'clustering':\n",
    "        # Silhouette score requires fitting first\n",
    "        pipeline.fit(X)\n",
    "        labels = pipeline.named_steps['model'].labels_\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        metrics = {'Silhouette Score': silhouette}\n",
    "    else:\n",
    "        metrics = {}\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 1.0, 'Precision': np.float64(1.0), 'Recall': np.float64(1.0), 'F1 Score': np.float64(1.0)}\n",
      "{'Mean Squared Error': np.float64(8.162001762548691e-07), 'Mean Absolute Error': np.float64(0.00018619975775340566), 'R2 Score': 0.9999993771408852}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "\n",
    "# Example for classification\n",
    "def test_pipeline_classification():\n",
    "    iris = load_iris(as_frame=True)\n",
    "    data = iris.frame\n",
    "    data['target'] = iris.target\n",
    "    data.to_csv('iris.csv', index=False)\n",
    "    \n",
    "    metrics = automated_ml_pipeline('iris.csv', 'classification')\n",
    "    print(metrics)\n",
    "\n",
    "# Example for regression\n",
    "def test_pipeline_regression():\n",
    "    california_housing = fetch_california_housing(as_frame=True)\n",
    "    data = california_housing.frame\n",
    "    data['target'] = california_housing.target\n",
    "    data.to_csv('california_housing.csv', index=False)\n",
    "    \n",
    "    metrics = automated_ml_pipeline('california_housing.csv', 'regression')\n",
    "    print(metrics)\n",
    "\n",
    "# Run tests\n",
    "test_pipeline_classification()\n",
    "test_pipeline_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
